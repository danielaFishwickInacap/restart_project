# Analyzing an IoT device data in real time

## Overview

Using an IoT device simulator, the data generated by an IoT device is collected, ingested, processed, analyzed and visualized in real time, through the implementation of a cloud-based solution.

The next cloud services are used to complete the solution presented in this project:

* AWS IoT Core
* Amazon Timestream
* Amazon Quicksight
* Grafana

## The Use Case

The next problem is used as a use case to elaborate the solution exposed in this project:

* A food services company trucks ice cream products to stores throughout the greater Santiago area.
* One of the biggest challenges in the distribution process is maintaining the cold chain.
* Over the past few months, their business has experienced 30 percent spoilage while the products are in transit.
* The problem could be with the following:
  * the truck refrigeration units turning off,
  * high summer temperatures,
  * truck doors being left open,
  * electrical issues, or
  * something they simply haven't thought of yet.

The company wants to use AWS to monitor various aspects of truck operation and determine the exact causes of the spoilage.

## Workflow to analyze and gain insights from IoT device data

IoT devices can generate an extensive amount of dataâ€”where does it all go? The following workflow describes how to analyze and gain insight from IoT device data, beginning with the initial data collection at the edge.

1. The workflow begins by IoT devices **collecting** data at the edge. Data could be stored and processed at the edge, depending on the use case.

2. Next, data is **ingested** to the AWS Cloud. A common way to do this is to use AWS IoT Device SDKs to connect your device and send data to AWS IoT Core.

3. When the data is in the AWS Cloud, it needs to be **stored** and **processed**.

4. Finally, **analyze** the stored data to visualize trends and gain insights.

![Data workflow](Images/image-5.png)

## The solution Architecture

The next diagram shown the solution proposed. Details of every steps of the data workflow process is explained in the chapters below.

![Architecture](<Images/iot project architecture v3 white bg.png>)

## Stage 1 and Stage 2: Collecting and Ingesting IoT Device Data

IoT devices and clients communicate with **messages** through the **MQTT protocol**. The information is gathered through the **publish-subscribe model**. A vital part of this model is the concept of **topic**.

The topic filters and selects messages for reception. The name of the topics could be defined by the user, each one associated with some data point to publish-subscribe model.

> A **topic** is a string that identifies a particular message channel in MQTT.  Topics are hierarchical and use a forward slash (/) as a delimiter. For example, a topic might be "home/living-room/temperature" where "home" is the top-level topic, "living-room" is a sub-topic, and "temperature" is a further sub-topic. 

The IoT device sends data to AWS using the **AWS IoT Device SDKs**. The AWS IoT Device SDKs are a set of libraries that help to create applications that interact with **AWS IoT Core**, like the Python script used here.

A briefly overview of **key components** of AWS IoT Core used in this project are:

* **Device gateway**: The secure entry point for IoT devices connecting to AWS IoT Core.
* **Message broker**: A messaging agent that securely transmits messages to and from IoT devices and applications.
* **Security and identity**: The requirements for authentication and authorization of devices and clients.

The next diagram resume the mentioned above:

![Key component of IoT core](Images/image-15.png)

The task of AWS IoT Message Broker is how IoT devices, clients and AWS IoT Core communicate. Devices send data by **publishing** a message on a topic. Clients receive messages by **subscribing** to a topic (The Publish/Subscribe Model).

In this project, an IoT simulator is used to send the metrics. Those are generated using a Python script in a Linux environment outside the AWS cloud.

The metrics simulated of the ice delivery truck were the following:

1. Freezer unit Voltage
2. Freezer door status (open/close)
3. Freezer Temperature
4. Date/Time stamp

![Truck sensors](Images/image-7.png)

Using the SDK, a connection to **AWS IoT Core** is created to send data as a **messages** to a **topic**, through the **MQTT protocol**. The messages **published** on a topic are received by the topic's **subscribers**.

![Collecting and ingesting the data](Images/image-8.png)

The picture above shown in a high level how the data is **collected** and then **ingested** by the IoT Core.

Next are the details of the steps done in the Stage 1:

### Step 1: Install the sensor (IoT simulator)

An Amazon Linux 2 dev environment was used to simulate an IoT device for the metrics generation and run terminal commands for that device. A Python script and a data set that contain the simulated metric were used too. Those files could be find in this repository in the *TruckSensorFiles* folder.

### Step 2: Provisioning the sensor

Connect the devices to the cloud, formally create and register a **thing**, which is a virtual representation of the physical device in the cloud.

The *Connect device wizard* was used in the AWS Management Console to achieve the following:

* Create a **thing type** and Add attributes to that thing type

![Thing type](Images/image-9.png)

* Create the **thing** representing the sensor and its **attributes**

![Thing 1](Images/image-10.png)
![Thing 2](Images/image-11.png)

### Step 3: Applying Security Certificates to a Thing and AWS IoT Policy

The device Authorization and Authentication consist briefly, as detailed next:

1. Device authentication (verifying the identity)
   1. X.509 certificate, using AWS Certificate Manager.
2. Device authorization (granting permissions)
   1. AWS IoT policies
      * attached to X.509 certificates
      * are JSON documents that contain a set of permissions
      * policy statement contains the following three keys:
         * **Effect**: Indicates whether the policy allows or denies access.
         * **Action**: Includes a list of actions that the policy allows or denies.
         * **Resource**: Specifies a list of resources to which the actions apply. If no resource is listed, then the action applies to the resource to which the policy is attached.

The *connection kit* provided by the AWS Management Console when the thing was created is used. It provides the device **certificate**, **public key**, and **private key**, in addition to an initial script that can be used to test connectivity with AWS IoT Core. All of those files were placed in the dev environment.

![Securing the device](Images/image-12.png)

Also, setting up the **AWS IoT policy** was done:

![Policy](Images/image-13.png)

The JSON policy:

![JSON policy](Images/image-14.png)

This AWS IoT policy JSON document contains two policy statements to allow actions of *iot:Connect* and *iot:Publish* for a single resource each.

### Step 4: Establishing communication

Now that the thing has the right AWS IoT policy in place and certificates are set up, the next step is to start the IoT device simulator script and begin communicating with AWS IoT Core.

In this demonstration, an MQTT topic is subscribed and verified the communications to the device. In our scenario, we subscribe to a topic published from our virtual sensor from the ice cream truck.

There are two stages in the demonstration:

* Generating messages from the thing.
* Subscribing to the topic in the AWS console.

#### A. Generating messages from the thing

In this stage of the demonstration, we return to the dev environment that was created earlier and launch the Python script. This script emulates an IoT device generating messages on a regular basis in a frequency of 2 messages per second and publishing the messages on the *truck/freezer* topic.

1. Launching the script

2. Confirm script is generating the messages: The script has connected to AWS IoT Core and is publishing messages to the truck/freezer topic, as shown the console output below:

```console
bash-5.2# ./start.sh 
Enter the device name > truckSensor01

Running truck sensor sample application...
Publishing messages on topic: truck/freezer
{"uptime":"1","volts":"12.11","temp":"21.90","door":"0","i":"1","n":"1","timestamp":1729703643.979735}
{"uptime":"4","volts":"12.11","temp":"21.82","door":"0","i":"1","n":"2","timestamp":1729703646.1263049}
{"uptime":"7","volts":"12.11","temp":"21.99","door":"0","i":"1","n":"3","timestamp":1729703648.2717602}
{"uptime":"10","volts":"12.11","temp":"21.90","door":"1","i":"1","n":"4","timestamp":1729703650.4197445}
{"uptime":"13","volts":"12.11","temp":"21.90","door":"0","i":"1","n":"5","timestamp":1729703652.5688615}
{"uptime":"16","volts":"12.08","temp":"21.90","door":"0","i":"1","n":"6","timestamp":1729703654.7161403}
{"uptime":"19","volts":"12.08","temp":"21.90","door":"0","i":"1","n":"7","timestamp":1729703656.8617609}
{"uptime":"22","volts":"12.08","temp":"21.82","door":"0","i":"1","n":"8","timestamp":1729703659.0104144}
{"uptime":"25","volts":"12.08","temp":"21.73","door":"1","i":"1","n":"9","timestamp":1729703661.1585238}
{"uptime":"28","volts":"12.08","temp":"21.82","door":"0","i":"1","n":"10","timestamp":1729703663.3042824}
{"uptime":"31","volts":"12.08","temp":"21.82","door":"1","i":"1","n":"11","timestamp":1729703665.4486833}
{"uptime":"34","volts":"12.08","temp":"21.73","door":"1","i":"1","n":"12","timestamp":1729703667.5955193}
{"uptime":"37","volts":"12.08","temp":"21.30","door":"0","i":"1","n":"13","timestamp":1729703669.7387779}
```

#### B. Subscribing to the topic in AWS IoT console

After generating messages from the IoT thing, confirming that AWS IoT Core can receive these messages. This is done in this step within the AWS IoT console, in the MQTT test page.

The data in each message contain the following:

* How long this device has been running
* The temperature in the back of the truck
* The power going to the freezer unit
* A timestamp in epoch format

Seeing these messages confirms that the communication between the device and AWS IoT Core is working, as shown the pictures below:

![MQTT test client 1](Images/image-6.png)
![MQTT test client 2](Images/image-36.png)

### Step 5: Adding rules to manage the traffic of messages

#### A. Creating the rule "truckAnalyze"

##### 1. Specify rule properties

![Rules properties](Images/image.png)

##### 2. Configure SQL statement

This statement transforms and standardizes the data for temperature and voltage by using a decimal number format. The statement formats the door open or closed indicator as an integer, and it transforms the epoch time format into a more human-readable format.

![Config SQL statement](Images/image-1.png)

##### 3. Attach rule actions

![Rules actions](Images/image-2.png)

##### 4. Creating an IAM Role to the action

An IAM Role is needed to giving permission access to IoT Core.

![Action IAM Rule](Images/image-37.png)

#### B. Checking the MQTT test messages running through a rule

##### 1. Subscribing to a new topic: truck/analyze

Subscribing to the new topic created above linked to the *truckAnalyze* rule.

![Subscribing to truck/analyze topic](Images/image-3.png)

##### 2. Checking the messages from the rule *truckAnalyze*

Verifying that the four data points that were included in the rule query statement are being sent to truck/analyze. The four data points were as follows:

* Temperature
* Voltage
* The door (whether it's opened or closed)
* A timestamp (converted from the epoch time format into a more human-readable value)

![truck/analyze messages](Images/image-4.png)

Finishing this step, conclude the implementation of the resources needed to Collect and Ingest the IoT data to AWS cloud.

## Stage 3: Storing and process data in the AWS Cloud

This project uses Amazon TimeStream to storage the data generated by the truck sensor. The capabilities of TimeStream to manage big amount of data and time series data, is the best choice for our case. Next a briefly description of TimeStream:

> **Amazon TimeStream**  is a fast, scalable, and serverless time series database service for IoT and operational applications that makes it easy to store and analyze trillions of events per day up to 1,000 times faster and at as little as 1/10th the cost of relational databases. Amazon TimeStream has built-in time series analytics functions, helping you identify trends and patterns in your data in near real-time. Amazon TimeStream is serverless and automatically scales up or down to adjust capacity and performance, so you don't need to manage the underlying infrastructure, freeing you to focus on building your applications.

### Part 1: Create a TimeStream Database

#### a. Create a database

The next picture shown the dialog box displaying the parameters defined to create the database:

![Creating the DB](Images/image-17.png)

The database created was a *Standard DB*

![Creating the DB](Images/image-18.png)

#### b. Create the Table

The next are the table configs used to create it:

![Create table 1](Images/image-19.png)
![Create table 2](Images/image-20.png)
![Create table 3](Images/image-21.png)

### Part 2: Creating a second rule action in AWS IoT core toward Amazon Timestream

It's time to transform and redirect the data to **Amazon Timestream**, adding a new rule action that directs the messages from the truck/analyze MQTT topic to Amazon Timestream.

#### A. Adding a rule action

Adding a new rule action to the rule *truckAnalyze* (created in the last stage) is shown in the next picture:

![Creating second rule action](Images/image-16.png)

The IAM role (*TimeStream_IoTRules_role*) created to give permission to this action to access Timestream is the next:

![TimeStream_IoTRules_role](Images/image-38.png)

The SQl statement, that perform some process of the raw data received from the truck sensors, was defined previously in the Stage 1-2, Step 5. Here is the defined statement:

![Rule SQL statement](Images/image-22.png)

An *Error action* is created as well, that will be executed when something goes wrong with processing the rule action, redirecting the messages to a new IoT Core Topic called *TimeStream_rule_error*.

![Error action](Images/image-23.png)

#### B. Verifying TimeStream database

Now, we could check if the data is going to TimeStream. In order to do that, a query is executed to check if TimeStream is receiving the data from IoT Core, using the built in *Query Editor* of TimeStream.

![Query editor 1](Images/image-24.png)
![Query editor 2](Images/image-25.png)

## Stage 4: Analyzing the data using a visualization-related AWS and third-party services

This project uses the self managed version of **Grafana** and **Amazon Quicksight** as a visualization tools. Both tools have the capabilities to create interactive dashboards. But, Grafana is best suitable for near Real-time analysis and Quicksight is useful for Batch or Schedule analysis.

> **Real-time analysis**: When you are collecting and analyzing streaming data as it occurs. **Near Real-time analysis** is similar but the data is analyzed in a range of seconds later the streaming data is received.

> **Scheduled analysis**: When the analysis is performed after the data has been gathered and processed.

### A. Grafana

Grafana is a popular open-source analytics platform where you can query, visualize, and create alerts on your metrics, logs, and traces. To install Grafana on your own, follow the [Install Grafana steps](https://https://grafana.com/docs/grafana/latest/setup-grafana/installation/) from the Grafana website. Grafana is suitable to use when you need real-time analysis or near real-time analysis.

#### 1. Install Amazon TimeStream plugin

Before adding TimeStream as our data source, a plugin is needed to be installed. More info could be find in the [plugin website](https://grafana.com/grafana/plugins/grafana-timestream-datasource/).

#### 2. Adding the data source

Now, we're going to load the TimeStream sensors data into Grafana. We add our TimeStream data source in the data source page, as shown the picture below:

![Adding Grafana data source](Images/image-26.png)

Next, authentication configuration are done as shown as follow:

![alt text](Images/image-27.png)

Specifying the IAM Access Keys is needed and the region where are located the resources. Also, *sensorDB* database, *truck01_sensors* table and a default measure must be choose as default macros.

#### 3. Creating the dashboard

A dashboard named *Truck01 sensors analytics* is created to show the metrics received from the truck sensors. The dashboard was built using the next visualizations:

* One **Time Series** visualization: that is a time based line chart showing the 3 metrics obtained from the freezer truck: temperature (green line), voltage (yellow line) and door status (blue line).

* Five **Stats** cards, showing the current and calculated temperature and voltage values: AVG (average), MAX (max value in the series), MIN (min value in the series). The last stat card show a counter of the door open status.

The picture below shown the created dashboard:

![Grafana dashboard](Images/image-28.png)

There are other capabilities of Grafana not shown here like creating Alerts when some metrics reach a specific value or range.

### B. Amazon QuickSight

Amazon QuickSight is an analytics service used to visualize data, perform analysis, and deliver insights. QuickSight is typically used to access data aggregates over time and dimensions for **scheduled analysis**. You can receive data from multiple data sources without needing to combine it outside of QuickSight.

#### 1. Adding the data source

Like the configs done in Grafana, adding TimeStream database as data source is needed to access the sensors data.

The next picture shown the data source selection in QuickSight:

![QuickSight data sources](Images/image-29.png)

The *sampleDB* database is chosen.

#### 2. Creating the dashboard

To create the dashboard, in the home panel, choose the "New analysis" button:

![New QuickSight analysis](Images/image-30.png)

Now select the *truck01_sensors* table:

![Selecting table](Images/image-31.png)

Selecting the table shown some details of it. Here, press "USE IN ANALYSIS" button:

![Using the table data for analysis](Images/image-32.png)

In the next dialog, just press "CREATE":

![Create the analysis](Images/image-33.png)

The dashboard created have just one visualization: a line chart showing the analysis of the entire dataset sent to IoT core from the IoT simulator:

![QuickSight dashboard](Images/image-34.png)

Like the Time series visualization of Grafana, the line chart shown the 3 metrics obtained from the freezer truck: temperature (green line), voltage (yellow line) and door status (blue line)

Analyzing the chart, is possible to conclude that the freezer device is not working properly after 15 minutes of reaching the working temperature of -18 Â°C. After that time, the temperature ascends until reaching the external temperature. Also, the voltage values shown that the working value is +/- 12 v. When the temperature start to ascend, the voltage value shown a little fall of 0.1 v (this fall is not clearly to see in the QuickSight dashboard due to the scale of the chart, but using the Grafana dashboard it is clearly noticeable in the picture below). Could be this the answer to the problem of the spoilage of this truck?

![Grafana dashboard showing details](Images/image-35.png)

Also, looking at the stats cards is possible to notice the MAX voltage value is 12.1 v and the MIN value is 11.8 v.

## Conclusion

The request asked to our team about the solution to the problematic of the Ice cream Company spoilage occurring during the delivery of its products could be associated to an electrical issue due to a power starving (voltage) of the delivery truck freezer device. Further analysis of the rest of the delivery trucks must be done in order to establish a clear cause of the origin of the presented problem. Also, additional measures must be collected to analyze another causes that could be affecting the freezer operating temperature, like the external temperature: the problematic happen only in hot days?; and the GPS location of the trucks: temperature going out of range when the truck is stuck in traffic, at a specific customer site, or cross-docking at a service center?

In other hand, long term analysis of gathered sensor data of the all delivery truck fleet could give insights and predictive analysis useful to improve the delivery process and business value.

## References

1. [Set up Amazon Linux 2 (AL2) in Windows Subsystem for Linux (WSL)](https://docs.aws.amazon.com/simspaceweaver/1.15/userguide/setting-up_local_wsl.html)

2. [AmazonWSL](https://github.com/yosukes-dev/AmazonWSL)

3. [AWS skill builder](https://skillbuilder.aws/)
   1. [Getting Started with AWS IoT - Digital training](https://explore.skillbuilder.aws/learn/course/internal/view/elearning/11841/getting-started-with-aws-iot)
   2. [Securely Connecting AWS IoT Devices to the Cloud - Digital training](https://explore.skillbuilder.aws/learn/course/15549/securely-connecting-aws-iot-devices-to-the-cloud)
   3. [Handling AWS IoT Device Data and States - Digital training](https://explore.skillbuilder.aws/learn/course/15379/handling-aws-iot-device-data-and-states)
   4. [Analyzing, Visualizing, and Gaining Insights from IoT Devices - Digital training](https://explore.skillbuilder.aws/learn/course/internal/view/elearning/15793/analyzing-visualizing-and-gaining-insights-from-iot-devices)
   5. [Introduction to AWS Internet of Things (IoT) - Self-Paced Lab](https://explore.skillbuilder.aws/learn/course/internal/view/elearning/588/introduction-to-aws-internet-of-things-iot)

4. [Unlocking Scalable IoT Analytics on AWS](https://aws.amazon.com/blogs/iot/unlocking-scalable-iot-analytics-on-aws)

5. [Deleting your Amazon QuickSight subscription and closing the account](https://docs.aws.amazon.com/quicksight/latest/user/closing-account.html)

6. [7 patterns for IoT data ingestion and visualization- How to decide what works best for your use case](https://aws.amazon.com/blogs/iot/7-patterns-for-iot-data-ingestion-and-visualization-how-to-decide-what-works-best-for-your-use-case/)

7. AWS workshop studio - AWS IoT Immersion Day Workshop, Amazon Timestream and Grafana, [Lab 135 - Timestream and Grafana](https://catalog.us-east-1.prod.workshops.aws/workshops/d144e5b2-21e8-4378-8ca1-b4847a485dbd/en-US/amazon-timestream/lab135-timestreamandgrafana)
